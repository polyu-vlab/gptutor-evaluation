{
    "question": "What is the previous state-of-the-art?",
    "ground_truth": "BART LARGE"
}{
    "question": "What is the architecture of the decoder?",
    "ground_truth": "M blocks, each consisting of self-attention module, context-attention module, and a two-layer feed-forward network."
}{
    "question": "What is the architecture of the encoder?",
    "ground_truth": "M blocks, each consisting of self-attention module and a two-layer feed-forward network."
}{
    "question": "What is the architecture of the saliency model?",
    "ground_truth": "M blocks, each consisting of a self-attention module and two-layer feed-forward network, combined with a  single-layer feed-forward network."
}