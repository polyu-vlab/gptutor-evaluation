{
    "question": "what dataset was used for training?",
    "ground_truth": "About 64M segments from YouTube videos comprising a total of 1.2B tokens."
}{
    "question": "what is the size of the training data?",
    "ground_truth": "64M video segments with 1.2B tokens"
}{
    "question": "what features were derived from the videos?",
    "ground_truth": "1500-dimensional vectors similar to those used for large scale image classification tasks."
}