{
    "question": "Do they manually check all adversarial examples that fooled some model for potential valid examples?",
    "ground_truth": "Only 100 successfully adversarial examples were manually checked, not all of them."
}{
    "question": "Are all generated examples semantics-preserving perturbations to the original text?",
    "ground_truth": "While the models aim to generate examples which preserve the semantics of the text with minimal perturbations, the Random model randomly replaces a character, which may not preserve the semantics. "
}{
    "question": "What is success rate of fooling tested models in experiments?",
    "ground_truth": "Authors best attacking model resulted in dip in the accuracy of CNN-Word (IMDB) by 79.43% and CNN-Char (AG's News) model by 72.16%"
}{
    "question": "What models are able to be fooled for AG's news corpus news categorization task by this approach?",
    "ground_truth": "A word-based convolutional model (CNN-Word) and a character-based convolutional model (CNN-Char)"
}