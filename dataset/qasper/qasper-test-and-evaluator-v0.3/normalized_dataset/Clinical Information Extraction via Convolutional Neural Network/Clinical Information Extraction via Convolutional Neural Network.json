{
    "question": "By how much did their model outperform baselines?",
    "ground_truth": "Answer with content missing: (Table 3) Best proposed result has F1 score of 0.844, 0.813, 0.870, 0.842, 0.844 compared to 0.855, 0.789, 0.852, 0.792, 0.833 on span, modality, degree, polarity and type respectively."
}{
    "question": "Which baselines did they compare against?",
    "ground_truth": "memorization, median report, max report"
}{
    "question": "What was their performance on this task?",
    "ground_truth": "Their average F1 score was 0.874 on span detection; 08115 on contextual modality detection; 0.8695 on degree detection; 0.839 on polarity detection; 0.844 on type detection"
}{
    "question": "How did they obtain part-of-speech tags?",
    "ground_truth": "Answer with content missing: (We then use ”PerceptronTagger” as our part-ofspeech tagger due to its fast tagging speed) PerceptronTagger."
}